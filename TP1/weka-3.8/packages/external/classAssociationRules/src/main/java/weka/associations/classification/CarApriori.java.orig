/*
 *   This program is free software: you can redistribute it and/or modify
 *   it under the terms of the GNU General Public License as published by
 *   the Free Software Foundation, either version 3 of the License, or
 *   (at your option) any later version.
 *
 *   This program is distributed in the hope that it will be useful,
 *   but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *   GNU General Public License for more details.
 *
 *   You should have received a copy of the GNU General Public License
 *   along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

/*
 *    CarApriori.java
 *    Copyright (C) 2004 Stefan Mutter
 *
 */

package weka.associations.classification;

import java.io.Reader;
import java.io.FileReader;
import java.io.BufferedReader;
import java.util.Enumeration;
import java.util.Hashtable;
import java.lang.Math;
import weka.core.Instances;
import weka.core.Instance;
import weka.core.FastVector;
import weka.core.OptionHandler;
import weka.core.Option;
import weka.core.Utils;
import weka.core.Tag;
import weka.core.SelectedTag;
import weka.core.AttributeStats;
import weka.filters.Filter;
import weka.filters.unsupervised.attribute.Remove;
import weka.associations.Apriori;
import weka.core.UnassignedClassException;
import weka.associations.ItemSet;

/**
 * Class implementing an Apriori-type algorithm to mine class association rules. 
 * Iteratively reduces the minimum
 * support until it finds all class association rules with the given minimum 
 * confidence. <p>
 *
 * Reference: R. Agrawal, R. Srikant (1994). <i>Fast algorithms for
 * mining association rules in large databases </i>. Proc
 * International Conference on Very Large Databases,
 * pp. 478-499. Santiage, Chile: Morgan Kaufmann, Los Altos, CA. <p>
 *
 * Reference: B. Liu, W. Hsu, Y. Ma (1998) <i>Integrating 
 * Classification and Association Rule Mining </i>. Proc
 * of the 4th Int. Conf. on Knowledge Discovery and Data Mining,
 * pp. 80-86. The AAAI Press. <p>
 *
 * Valid options are:<p>
 *   
 * -N required number of rules <br>
 * The required number of rules (default: not resrricted). <p>
 *
 * -C minimum confidence of a rule <br>
 * The minimum confidence of a rule (default: 0.5). <p>
 *
 * -D delta for minimum support <br>
 * The delta by which the minimum support is decreased in
 * each iteration (default: 0.01). <p>
 *
 * -U upper bound for minimum support <br>
 * The upper bound for minimum support. Don't explicitly look for 
 * rules with more than this level of support. <p>
 *
 * -M lower bound for minimum support <br>
 * The lower bound for the minimum support (default = 0.1). <p>
 *
 * -R <br>
 * If set then columns that contain all missing values are removed from
 * the data.
 *
 * -I <br>
 * If set the itemsets found are also output (default = no). <p>
 *
 * -c class index <br>
 * Sets the class attribute. (default last)
 * Note: When used with a classifier, the classifier's class attribute setting overrides this parameter. <p>
 *
 *
 * @author Stefan Mutter (mutter@cs.waikato.ac.nz)
 * @version $Revision$ */
public class CarApriori extends Apriori implements OptionHandler, CARuleMiner {
  
  /** Only the class attribute of all Instances.*/
  protected Instances m_onlyClass;
  
  /** The class index. */  
  protected int m_classIndex;

  

  /**
   * Returns a string describing this associator
   * @return a description of the evaluator suitable for
   * displaying in the explorer/experimenter gui
   */
  public String globalInfo() {
    return "Finds class association rules.";
  }

  /**
   * Constructor that allows to sets default values for the 
   * minimum confidence and the maximum number of rules
   * the minimum confidence.
   */
  public CarApriori() {

    resetOptions();
  }

  /**
   * Resets the options to the default values.
   */
  public void resetOptions() {
    
    m_removeMissingCols = false;
    m_verbose = false;
    m_delta = 0.05;
    m_minMetric = 0.5;
    m_numRules = Integer.MAX_VALUE;
    m_lowerBoundMinSupport = 0.01;
    m_upperBoundMinSupport = 1.0;
    m_significanceLevel = -1;
    m_outputItemSets = false;
    m_classIndex = -1;
    
  }

  /**
   * Method that generates all large itemsets with a minimum support, and from
   * these all class association rules with a minimum confidence.
   *
   * @param instances the instances to be used for generating the associations
   * @exception Exception if rules can't be built successfully
   */
  public void buildAssociations(Instances instances) throws Exception {

    double[] confidences, supports;
    int[] indices;
    FastVector[] sortedRuleSet, sortedRuleSet2;
    int necSupport=0;
    

    if (instances.checkForStringAttributes()) {
      throw new Exception("Can't handle string attributes!");
    }
    if(m_metricType != CONFIDENCE)
      throw new Exception("For CAR-Mining metric type has to be confidence!");
    
    if (m_removeMissingCols) {
      instances = removeMissingColumns(instances);
    }
    if(m_classIndex == -1)
        instances.setClassIndex(instances.numAttributes()-1);     
    else
        if(m_classIndex < instances.numAttributes() && m_classIndex >= 0)
            instances.setClassIndex(m_classIndex);
        else
            throw new Exception("Invalid class index.");
    
    m_cycles = 0;
    
    //m_instances does not contain the class attribute
    m_instances = LabeledItemSet.divide(instances,false);
    
    //m_onlyClass contains only the class attribute
    m_onlyClass = LabeledItemSet.divide(instances,true);
    
    if(m_numRules == Integer.MAX_VALUE){
        // Set desired minimum support
        m_minSupport = m_lowerBoundMinSupport;
    }
    else{
        // Decrease minimum support until desired number of rules found.
        m_minSupport = m_upperBoundMinSupport - m_delta;
        m_minSupport = (m_minSupport < m_lowerBoundMinSupport) 
            ? m_lowerBoundMinSupport 
            : m_minSupport;
    }
    
    do {
      m_Ls = new FastVector();
      m_hashtables = new FastVector();
      m_allTheRules = new FastVector[6];
      m_allTheRules[0] = new FastVector();
      m_allTheRules[1] = new FastVector();
      m_allTheRules[2] = new FastVector();
      if (m_metricType != CONFIDENCE || m_significanceLevel != -1) {
	m_allTheRules[3] = new FastVector();
	m_allTheRules[4] = new FastVector();
	m_allTheRules[5] = new FastVector();
      }
      sortedRuleSet = new FastVector[6];
      sortedRuleSet[0] = new FastVector();
      sortedRuleSet[1] = new FastVector();
      sortedRuleSet[2] = new FastVector();
      if (m_metricType != CONFIDENCE || m_significanceLevel != -1) {
	sortedRuleSet[3] = new FastVector();
	sortedRuleSet[4] = new FastVector();
	sortedRuleSet[5] = new FastVector();
      }
      findLargeItemSets(instances);
      findRulesQuickly();
      
      // Sort rules according to their support
      int j = m_allTheRules[2].size()-1;
      supports = new double[m_allTheRules[2].size()];
      for (int i = 0; i < (j+1); i++) 
	supports[j-i] = ((double)((ItemSet)m_allTheRules[1].elementAt(j-i)).support())*(-1);
      indices = Utils.stableSort(supports);
      for (int i = 0; i < (j+1); i++) {
	sortedRuleSet[0].addElement(m_allTheRules[0].elementAt(indices[j-i]));
	sortedRuleSet[1].addElement(m_allTheRules[1].elementAt(indices[j-i]));
	sortedRuleSet[2].addElement(m_allTheRules[2].elementAt(indices[j-i]));
	if (m_metricType != CONFIDENCE || m_significanceLevel != -1) {
	  sortedRuleSet[3].addElement(m_allTheRules[3].elementAt(indices[j-i]));
	  sortedRuleSet[4].addElement(m_allTheRules[4].elementAt(indices[j-i]));
	  sortedRuleSet[5].addElement(m_allTheRules[5].elementAt(indices[j-i]));
	}
      }

      // Sort rules according to their confidence
      m_allTheRules[0].removeAllElements();
      m_allTheRules[1].removeAllElements();
      m_allTheRules[2].removeAllElements();
      if (m_metricType != CONFIDENCE || m_significanceLevel != -1) {
	m_allTheRules[3].removeAllElements();
	m_allTheRules[4].removeAllElements();
	m_allTheRules[5].removeAllElements();
      }
      confidences = new double[sortedRuleSet[2].size()];
      int sortType = 2 + m_metricType;

      for (int i = 0; i < sortedRuleSet[2].size(); i++) 
	confidences[i] = 
	  ((Double)sortedRuleSet[sortType].elementAt(i)).doubleValue();
      indices = Utils.stableSort(confidences);
      for (int i = sortedRuleSet[0].size() - 1; 
	   (i >= (sortedRuleSet[0].size() - m_numRules)) && (i >= 0); i--) {
	m_allTheRules[0].addElement(sortedRuleSet[0].elementAt(indices[i]));
	m_allTheRules[1].addElement(sortedRuleSet[1].elementAt(indices[i]));
	m_allTheRules[2].addElement(sortedRuleSet[2].elementAt(indices[i]));
	if (m_metricType != CONFIDENCE || m_significanceLevel != -1) {
	  m_allTheRules[3].addElement(sortedRuleSet[3].elementAt(indices[i]));
	  m_allTheRules[4].addElement(sortedRuleSet[4].elementAt(indices[i]));
	  m_allTheRules[5].addElement(sortedRuleSet[5].elementAt(indices[i]));
	}
      }
      
      if (m_verbose) {
	  if (m_Ls.size() > 1) {
	      System.out.println(toString());
	  }
      }
      
      if(m_minSupport == m_lowerBoundMinSupport || m_minSupport - m_delta >  m_lowerBoundMinSupport)
        m_minSupport -= m_delta;
      else
        m_minSupport = m_lowerBoundMinSupport;
      
      necSupport = Math.round((float)((m_minSupport * 
			 (double)instances.numInstances())+0.5));
      
      
      m_cycles++;
      
    }while ((m_allTheRules[0].size() < m_numRules) &&
	     (Utils.grOrEq(m_minSupport, m_lowerBoundMinSupport))
	     /*	     (necSupport >= lowerBoundNumInstancesSupport)*/
	     /*	     (Utils.grOrEq(m_minSupport, m_lowerBoundMinSupport)) */ &&     
	     (necSupport >= 1));
    m_minSupport += m_delta;
    
  }


    /**
     * Method that mines all class association rules with minimum support and
     * with a minimum confidence.
     * @return an sorted array of FastVector (confidence depended) containing the rules and metric information
     * @param data the instances for which class association rules should be mined
     * @exception Exception if rules can't be built successfully
     */
    public FastVector[] mineCARs(Instances data) throws Exception{
	 
	buildAssociations(data);
	return m_allTheRules;
    }

   /**
   * Gets the instances without the class atrribute.
   *
   * @return the instances without the class attribute.
   */ 
  public Instances getInstancesNoClass() {
      
      return m_instances;
  }  
  
  
  /**
   * Gets only the class attribute of the instances.
   *
   * @return the class attribute of all instances.
   */ 
  public Instances getInstancesOnlyClass() {
      
      return m_onlyClass;
  }  

    
  /**
   * Returns an enumeration describing the available options.
   *
   * @return an enumeration of all the available options.
   */
  public Enumeration listOptions() {

    String string1 = "\tThe required number of rules. (default: not restricted)",
      string2 = 
      "\tThe minimum confidence of a rule. (default = " + m_minMetric + ")",
      string3 = "\tThe delta by which the minimum support is decreased in\n",
      string4 = "\teach iteration. (default = " + m_delta + ")",
      string5 = 
      "\tThe lower bound for the minimum support. (default = " + 
      m_lowerBoundMinSupport + ")",
      string6 = "\tIf set the itemsets found are also output. (default = no)",
      string7 = "\tSets the class attribute. (default last) first and last are valid indices.\n\tNote: When used with a classifier, the classifier's class attribute setting overrides this parameter.";;
    

    FastVector newVector = new FastVector(9);

    newVector.addElement(new Option(string1, "N", 1, 
				    "-N <required number of rules output>"));
    newVector.addElement(new Option(string2, "C", 1, 
				    "-C <minimum metric score of a rule>"));
    newVector.addElement(new Option(string3 + string4, "D", 1,
				    "-D <delta for minimum support>"));
    newVector.addElement(new Option("\tUpper bound for minimum support. "
				    +"(default = 1.0)", "U", 1,
				     "-U <upper bound for minimum support>"));
    newVector.addElement(new Option(string5, "M", 1,
				    "-M <lower bound for minimum support>"));
    newVector.addElement(new Option(string6, "S", 0,
				    "-I"));
    newVector.addElement(new Option("\tRemove columns that contain "
				    +"all missing values (default = no)"
				    , "R", 0,
				    "-R"));
    newVector.addElement(new Option("\tReport progress iteratively. (default "
				    +"= no)", "V", 0,
				    "-V"));
    
    newVector.addElement(new Option(string7, "c", 1, 
				    "-c <class indes>"));
    
    return newVector.elements();
  }

  /**
   * Parses a given list of options. Valid options are:<p>
   *   
   * -N required number of rules <br>
   * The required number of rules (default: 10). <p>
   *
   * -C minimum metric score of a rule <br>
   * The minimum confidence of a rule (default: 0.9). <p>
   *
   * -D delta for minimum support <br>
   * The delta by which the minimum support is decreased in
   * each iteration (default: 0.05).
   *
   * -U upper bound for minimum support <br>
   * The upper bound for minimum support. Don't explicitly look for 
   * rules with more than this level of support. <p>
   *
   * -M lower bound for minimum support <br>
   * The lower bound for the minimum support (default = 0.1). <p>
   *
   * -I <br>
   * If set the itemsets found are also output (default = no). <p>
   *
   * -V <br>
   * If set then progress is reported iteratively during execution. <p>
   *
   * -R <br>
   * If set then columns that contain all missing values are removed from
   * the data. <p>
   *
   * -c class index <br>
   * Sets the class attribute. (default last)
   * Note: When used with a classifier, the classifier's class attribute setting overrides this parameter. <p>
   *
   * @param options the list of options as an array of strings
   * @exception Exception if an option is not supported 
   */
  public void setOptions(String[] options) throws Exception {
    
    resetOptions();
    String numRulesString = Utils.getOption('N', options),
      minConfidenceString = Utils.getOption('C', options),
      deltaString = Utils.getOption('D', options),
      maxSupportString = Utils.getOption('U', options),
      minSupportString = Utils.getOption('M', options),
      classIndexString = Utils.getOption('c', options);
      
    setMetricType(new SelectedTag(0,TAGS_SELECTION));
    
    if (numRulesString.length() != 0) {
            m_numRules = Integer.parseInt(numRulesString);
    }
    if (minConfidenceString.length() != 0) {
      m_minMetric = (new Double(minConfidenceString)).doubleValue();
    }
    if (deltaString.length() != 0) {
      m_delta = (new Double(deltaString)).doubleValue();
    }
    if (maxSupportString.length() != 0) {
      setUpperBoundMinSupport((new Double(maxSupportString)).doubleValue());
    }
    if (minSupportString.length() != 0) {
      m_lowerBoundMinSupport = (new Double(minSupportString)).doubleValue();
    }
    m_outputItemSets = Utils.getFlag('I', options);
    m_verbose = Utils.getFlag('V', options);
    setRemoveAllMissingCols(Utils.getFlag('R', options));
    if (classIndexString.length() != 0){
        if(classIndexString.equals("first"))
            m_classIndex = 0;
        else
            if(classIndexString.equals("last"))
                m_classIndex = -1;
            else    
                m_classIndex = Integer.parseInt(classIndexString);
    }
    else
        m_classIndex = -1;
  }

  /**
   * Gets the current settings of the CarApriori object.
   *
   * @return an array of strings suitable for passing to setOptions
   */
  public String [] getOptions() {

    String [] options = new String [20];
    int current = 0;

    if (m_outputItemSets) {
      options[current++] = "-I";
    }

    if (getRemoveAllMissingCols()) {
      options[current++] = "-R";
    }

    options[current++] = "-N"; options[current++] = "" + m_numRules;
    options[current++] = "-C"; options[current++] = "" + m_minMetric;
    options[current++] = "-D"; options[current++] = "" + m_delta;
    options[current++] = "-U"; options[current++] = ""+m_upperBoundMinSupport;
    options[current++] = "-M"; options[current++] = ""+m_lowerBoundMinSupport;
    if(m_classIndex == -1){
       options[current++] = "-c"; options[current++] = "" + "last";
    }
    else{
        options[current++] = "-c"; options[current++] = "" + m_classIndex;
    }

    while (current < options.length) {
      options[current++] = "";
    }
    return options;
  }

  /**
   * Sets the class index
   * @param index the class index
   */  
  public void setClassIndex(int index){
      
      m_classIndex = index;
  }
  
  /**
   * Gets the class index
   * @return the index of the class attribute
   */  
  public int getClassIndex(){
      
      return m_classIndex;
  }

  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String classIndexTipText() {
    return "Index of the class attribute.\n If set to -1, the last attribute will be taken as the class attribute.\nIf used in classification, the class attribute of the classifier overrides this option.";
  }

  /**
   * Outputs the size of all the generated sets of itemsets and the rules.
   * @return a string output of the class association rule mining results
   */
  public String toString() {

    StringBuffer text = new StringBuffer();

    if (m_Ls.size() <= 1)
      return "\nNo large itemsets and rules found!\n";
    text.append("\nApriori\n=======\n\n");
    text.append("Minimum support: " 
		+ Utils.doubleToString(m_minSupport,2) + '\n');
	text.append("Minimum metric <");
	switch(m_metricType) {
	case CONFIDENCE:
	    text.append("confidence>: ");
	    break;
	case LIFT:
	    text.append("lift>: ");
	    break;
	case LEVERAGE:
	    text.append("leverage>: ");
	    break;
	case CONVICTION:
	    text.append("conviction>: ");
	    break;
	}
	text.append(Utils.doubleToString(m_minMetric,2)+'\n');
    
    if (m_significanceLevel != -1)
      text.append("Significance level: "+
		  Utils.doubleToString(m_significanceLevel,2)+'\n');
    text.append("Number of cycles performed: " + m_cycles+'\n');
    text.append("\nGenerated sets of large itemsets:\n");
    for (int i = 0; i < m_Ls.size(); i++) {
      text.append("\nSize of set of large itemsets L("+(i+1)+"): "+
		  ((FastVector)m_Ls.elementAt(i)).size()+'\n');
      if (m_outputItemSets) {
	text.append("\nLarge Itemsets L("+(i+1)+"):\n");
	for (int j = 0; j < ((FastVector)m_Ls.elementAt(i)).size(); j++){
	  text.append(((ItemSet)((FastVector)m_Ls.elementAt(i)).elementAt(j)).
		      toString(m_instances)+"\n");
          //if(m_CARs){
            text.append(((LabeledItemSet)((FastVector)m_Ls.elementAt(i)).elementAt(j)).m_classLabel+"  ");
            text.append(((LabeledItemSet)((FastVector)m_Ls.elementAt(i)).elementAt(j)).support()+"\n");
          //}
	}
      }
    }
    text.append("\nBest rules found:\n\n");
    for (int i = 0; i < m_allTheRules[0].size(); i++) {
      text.append(Utils.doubleToString((double)i+1, 
					     (int)(Math.log(m_numRules)/Math.log(10)+1),0)+
			". " + ((ItemSet)m_allTheRules[0].elementAt(i)).
			toString(m_instances) 
			+ " ==> " + ((ItemSet)m_allTheRules[1].elementAt(i)).
			toString(m_onlyClass) +"    conf:("+  
			Utils.doubleToString(((Double)m_allTheRules[2].
					      elementAt(i)).doubleValue(),2)+")");
	
      if (m_metricType != CONFIDENCE || m_significanceLevel != -1) {
	text.append((m_metricType == LIFT ? " <" : "")+" lift:("+  
		    Utils.doubleToString(((Double)m_allTheRules[3].
					  elementAt(i)).doubleValue(),2)
		    +")"+(m_metricType == LIFT ? ">" : ""));
	text.append((m_metricType == LEVERAGE ? " <" : "")+" lev:("+  
		    Utils.doubleToString(((Double)m_allTheRules[4].
					  elementAt(i)).doubleValue(),2)
		    +")");
	text.append(" ["+
		    (int)(((Double)m_allTheRules[4].elementAt(i))
			  .doubleValue() * (double)m_instances.numInstances())
		    +"]"+(m_metricType == LEVERAGE ? ">" : ""));
	text.append((m_metricType == CONVICTION ? " <" : "")+" conv:("+  
		    Utils.doubleToString(((Double)m_allTheRules[5].
					  elementAt(i)).doubleValue(),2)
		    +")"+(m_metricType == CONVICTION ? ">" : ""));
      }
      text.append('\n');
    }
    return text.toString();
  }

  
  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String metricTypeTipText() {
    return "For class association rule mining the metric type has to be confidence";
  }

  /**
   * Set the metric type for ranking rules
   *
   * @param d the type of metric
   */
  public void setMetricType (SelectedTag d) {
    
    if (d.getTags() == TAGS_SELECTION) {
      m_metricType = d.getSelectedTag().getID();
    }

    if (m_significanceLevel != -1 && m_metricType != CONFIDENCE) {
      m_metricType = CONFIDENCE;
    }

    setMinMetric(0.5);
    
  }

  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String minMetricTipText() {
    return "Minimum metric score. Consider only rules with scores higher than "
      +"this value.";
  }

  /**
   * Get the value of minConfidence.
   *
   * @return Value of minConfidence.
   */
  public double getMinMetric() {
    
    return m_minMetric;
  }
  
  /**
   * Set the value of minConfidence.
   *
   * @param v  Value to assign to minConfidence.
   */
  public void setMinMetric(double v) {
    
    m_minMetric = v;
  }

  /**
   * Returns the tip text for this property
   * @return tip text for this property suitable for
   * displaying in the explorer/experimenter gui
   */
  public String numRulesTipText() {
    return "Number of rules to mine."
        + "If set to -1 the number of rules is not restricted";
  }

  /**
   * Get the value of numRules.
   *
   * @return Value of numRules.
   */
  public int getNumRules() {
      
    return m_numRules;
  }
  
  /**
   * Set the value of numRules.
   *
   * @param v  Value to assign to numRules.
   */
  public void setNumRules(int v) {
      if(v == -1)
	  m_numRules = Integer.MAX_VALUE;
      else
	  m_numRules = v;
  }
  


 
  /**
   * Returns the metric string for the chosen metric type
   * @return a string describing the used metric for the interestingness of a class association rule
   */
  public String metricString() {
      
        switch(m_metricType) {
	case LIFT:
	    return "lif";
	case LEVERAGE:
	    return "leverage"; 
	case CONVICTION:
	    return "conviction";
        default:
            return "conf";
	}
  }

    /**
     *
     * Method that finds all large itemsets for the given set of instances.
     * @param instances the instances for which large item sets are found
     * @exception Exception if an attribute is numeric
     */
    private void findLargeItemSets(Instances instances) throws Exception {
	
	FastVector kMinusOneSets, kSets;
	Hashtable hashtable;
	int necSupport, necMaxSupport,i = 0;
	
	// Find large itemsets
	
	// minimum support
        double nextMinSupport = m_minSupport*(double)instances.numInstances();
        double nextMaxSupport = m_upperBoundMinSupport*(double)instances.numInstances();
	if((double)Math.rint(nextMinSupport) == nextMinSupport){
            necSupport = (int) nextMinSupport;
        }
        else{
            necSupport = Math.round((float)(nextMinSupport+0.5));
        }
        if((double)Math.rint(nextMaxSupport) == nextMaxSupport){
            necMaxSupport = (int) nextMaxSupport;
        }
        else{
            necMaxSupport = Math.round((float)(nextMaxSupport+0.5));
        }
	
	//find item sets of length one
	kSets = LabeledItemSet.singletons(m_instances,m_onlyClass);
	LabeledItemSet.upDateCounters(kSets, m_instances,m_onlyClass);
        
        //check if a item set of lentgh one is frequent, if not delete it
	kSets = LabeledItemSet.deleteItemSets(kSets, necSupport, necMaxSupport);
        if (kSets.size() == 0)
	    return;
	do {
	    m_Ls.addElement(kSets);
	    kMinusOneSets = kSets;
	    kSets = LabeledItemSet.mergeAllItemSets(kMinusOneSets, i, instances.numInstances());
	    hashtable = LabeledItemSet.getHashtable(kMinusOneSets, kMinusOneSets.size());
	    kSets = LabeledItemSet.pruneItemSets(kSets, hashtable);
	    LabeledItemSet.upDateCounters(kSets, m_instances,m_onlyClass);
	    kSets = LabeledItemSet.deleteItemSets(kSets, necSupport, necMaxSupport);
	    i++;
	} while (kSets.size() > 0);
    } 

   

    /** 
   * Method that finds all class association rules.
   *
   * @exception Exception if an attribute is numeric
   */
   private void findRulesQuickly() throws Exception {

    FastVector[] rules;

    // Build rules
    for (int j = 0; j < m_Ls.size(); j++) {
      FastVector currentLabeledItemSets = (FastVector)m_Ls.elementAt(j);
      Enumeration enumLabeledItemSets = currentLabeledItemSets.elements();
      while (enumLabeledItemSets.hasMoreElements()) {
	LabeledItemSet currentLabeledItemSet = (LabeledItemSet)enumLabeledItemSets.nextElement();
	rules = currentLabeledItemSet.generateRules(m_minMetric,false);
	for (int k = 0; k < rules[0].size(); k++) {
	  m_allTheRules[0].addElement(rules[0].elementAt(k));
	  m_allTheRules[1].addElement(rules[1].elementAt(k));
	  m_allTheRules[2].addElement(rules[2].elementAt(k));
	}
      }
    }
  }

 
   


   /**
    * Main method for testing this class.
    * @param options the options for the rule miner
    */
  public static void main(String[] options) {

    String trainFileString;
    StringBuffer text = new StringBuffer();
    CarApriori apriori = new CarApriori();
    Reader reader;

    try {
      text.append("\n\nCarApriori options:\n\n");
      text.append("-t <training file>\n");
      text.append("\tThe name of the training file.\n");
      Enumeration enum = apriori.listOptions();
      while (enum.hasMoreElements()) {
	Option option = (Option)enum.nextElement();
	text.append(option.synopsis()+'\n');
	text.append(option.description()+'\n');
      }
      trainFileString = Utils.getOption('t', options);
      if (trainFileString.length() == 0) 
	throw new Exception("No training file given!");
      apriori.setOptions(options);
      reader = new BufferedReader(new FileReader(trainFileString));
      apriori.buildAssociations(new Instances(reader));
      System.out.println(apriori);
    } catch(Exception e) {
      e.printStackTrace();
      System.out.println("\n"+e.getMessage()+text);
    }
  }
  
  
  
}



