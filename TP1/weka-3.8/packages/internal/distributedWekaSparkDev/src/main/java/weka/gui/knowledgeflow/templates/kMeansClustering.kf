{
	"flow_name" : "runKMeansParallel",
	"steps" : [
		{
			"class" : "weka.knowledgeflow.steps.CSVDataSource",
			"properties" : {
				"datasourceOptions" : "-min-slices 4 -output-dir ${user.home}/sparkOutput -master local[*] -cluster-mem -1.0 -overhead 3.0 -mem-fraction 0.6 -charset UTF-8 -comment # -escape \\ -F , -csv-header -Q \"\\'\" -input-file ${WEKA_HOME}/packages/distributedWekaSparkDev/sample_data/hypothyroid_with_header_row.csv -dataset-type trainingData",
				"name" : "CSVDataSource"
			},
			"connections" : {
				"dataFrame" : [
					"ArffHeaderSparkJob"
				]
			},
			"coordinates" : "30,142"
		},
		{
			"class" : "weka.knowledgeflow.steps.ArffHeaderSparkJob",
			"properties" : {
				"jobOptions" : "-header-file-name hypo.arff -data-source weka.distributed.spark.CSVDataSource -debug -cluster-mem -1.0 -overhead 3.0 -mem-fraction 0.6 -charset UTF-8 -comment # -escape \\ -F , -M ? -Q \"\\'\" -dataset-type trainingData -compression 50.0 -decimal-places 2",
				"name" : "ArffHeaderSparkJob"
			},
			"connections" : {
				"success" : [
					"RandomlyShuffleDataSparkJob"
				]
			},
			"coordinates" : "190,142"
		},
		{
			"class" : "weka.knowledgeflow.steps.Note",
			"properties" : {
				"name" : "Note",
				"noteText" : "<html>Uses the hypothyroid_with_header_row \ncsv file in the <b>sample_data</b> directory of\nthe distributedWekaSpark package as input. \nThe dataset is split into 4 partitions, and \nan ARFF header with additional summary \nmetadata attributes is computed using all the\nCPU cores on your computer.</html>"
			},
			"connections" : {
			},
			"coordinates" : "28,248"
		},
		{
			"class" : "weka.knowledgeflow.steps.TextViewer",
			"properties" : {
				"name" : "TextViewer"
			},
			"connections" : {
			},
			"coordinates" : "668,142"
		},
		{
			"class" : "weka.knowledgeflow.steps.KMeansClustererSparkJob",
			"properties" : {
				"jobOptions" : "-model-file-name kMeans.model -num-clusters 4 -num-iterations 20 -num-runs 5 -init-kmeans-its 5 -seed 1 -tolerance 1.0E-4 -filter \"weka.filters.unsupervised.attribute.Remove -R last\" -data-source weka.distributed.spark.CSVDataSource -min-slices 1 -cluster-mem -1.0 -overhead 3.0 -mem-fraction 0.6 -charset UTF-8 -comment # -escape \\ -F , -M ? -Q \"\\'\" -dataset-type trainingData -compression 50.0 -decimal-places 2 -seed 1 -num-splits 10 -data-source weka.distributed.spark.CSVDataSource -min-slices 1 -cluster-mem -1.0 -overhead 3.0 -mem-fraction 0.6 -charset UTF-8 -comment # -escape \\ -F , -M ? -Q \"\\'\" -dataset-type trainingData -compression 50.0 -decimal-places 2",
				"name" : "KMeansClustererSparkJob"
			},
			"connections" : {
				"text" : [
					"TextViewer"
				]
			},
			"coordinates" : "508,142"
		},
		{
			"class" : "weka.knowledgeflow.steps.RandomizedDataSparkJob",
			"properties" : {
				"jobOptions" : "-seed 1 -num-splits 4 -data-source weka.distributed.spark.CSVDataSource -min-slices 1 -cluster-mem -1.0 -overhead 3.0 -mem-fraction 0.6 -charset UTF-8 -comment # -escape \\ -F , -M ? -Q \"\\'\" -dataset-type trainingData -compression 50.0 -decimal-places 2",
				"name" : "RandomlyShuffleDataSparkJob"
			},
			"connections" : {
				"success" : [
					"KMeansClustererSparkJob"
				]
			},
			"coordinates" : "348,142"
		},
		{
			"class" : "weka.knowledgeflow.steps.Note",
			"properties" : {
				"name" : "Note2",
				"noteText" : "Randomly shuffle and\nstratify the data (RDD) \ninto 4 partitions."
			},
			"connections" : {
			},
			"coordinates" : "314,256"
		},
		{
			"class" : "weka.knowledgeflow.steps.Note",
			"properties" : {
				"name" : "Note3",
				"noteText" : "Executes 5 separate runs\nof k-means++ and selects\nthe best (lowest within\ncluster SSE). Uses a Remove\nfilter to remove the class\nattribute from the data\non the fly."
			},
			"connections" : {
			},
			"coordinates" : "479,257"
		},
		{
			"class" : "weka.knowledgeflow.steps.Note",
			"properties" : {
				"name" : "Note22",
				"noteText" : "<html><b>NOTE: this flow is configured to run out of the box.</b> It stores output in ${user.home}/sparkOutput.\n\nThis flow demonstrates loading data into the Spark environment via Spark's data frame-based data sources (CSV \nin this case). The (potentially) big datasets are then processed by distributed Weka's strategies for dealing with\nlarge (larger than can fit into desktop RAM) datasets.</html>"
			},
			"connections" : {
			},
			"coordinates" : "20,18"
		}
	]
}
